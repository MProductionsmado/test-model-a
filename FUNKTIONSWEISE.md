# ğŸ§  Funktionsweise - Wie die Minecraft-KI arbeitet

## Ãœberblick

Diese KI ist ein **Conditional Transformer GPT** (Generative Pre-trained Transformer), der speziell fÃ¼r die Generierung von Minecraft-Strukturen aus Text-Beschreibungen trainiert wurde.

**Ein Satz-Zusammenfassung:**  
Die KI liest deinen Text-Prompt, versteht die Beschreibung durch einen Text-Encoder, und generiert dann Block-fÃ¼r-Block eine passende 16Ã—16Ã—16 Minecraft-Struktur durch Cross-Attention zwischen Text und Struktur-Tokens.

---

## ğŸ—ï¸ System-Architektur

### Komponenten-Ãœbersicht

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER INPUT: "a big medieval house with oak wood"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT TOKENIZER                                             â”‚
â”‚  â€¢ Zerlegt Text in WÃ¶rter: ["a","big","medieval","house"...]â”‚
â”‚  â€¢ Konvertiert zu Token-IDs: [5, 42, 78, 34, ...]          â”‚
â”‚  â€¢ Vokabular: 202 WÃ¶rter aus Training-Dateinamen           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEXT ENCODER (4-Layer Transformer)                         â”‚
â”‚  â€¢ Versteht semantische Bedeutung des Prompts              â”‚
â”‚  â€¢ Erzeugt Text-Embeddings (256-dimensional)               â”‚
â”‚  â€¢ Output: Kontext-Vektor fÃ¼r jeden Token                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONDITIONAL GPT MODEL (12-Layer Transformer)               â”‚
â”‚  â€¢ Generiert BlÃ¶cke autoregressiv (einer nach dem anderen) â”‚
â”‚  â€¢ Jeder Block berÃ¼cksichtigt:                             â”‚
â”‚    1. Alle bisher generierten BlÃ¶cke (Self-Attention)      â”‚
â”‚    2. Die Text-Beschreibung (Cross-Attention)              â”‚
â”‚  â€¢ 512-dimensional, 8 Attention-Heads                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT: 16Ã—16Ã—16 Minecraft-Struktur                        â”‚
â”‚  â€¢ 4096 BlÃ¶cke aus 121 mÃ¶glichen Block-Typen               â”‚
â”‚  â€¢ Gespeichert als .schem Datei                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¤ 1. Text-Verarbeitung

### 1.1 Text Tokenizer

**Aufgabe:** Konvertiert Text-Prompt in numerische Token.

**Beispiel:**
```
Input:  "a big medieval house with oak wood"
Tokens: [5, 42, 78, 34, 89, 15, 91]
        â†“  â†“   â†“   â†“    â†“   â†“   â†“
        a big med house with oak wood
```

**Vokabular-Aufbau:**
- Extrahiert aus 2415 Trainings-Dateinamen
- Beispiel-Dateiname: `a_big_abandoned_barn_built_out_of_spruce_wood_0001.schem`
- Underscores â†’ Leerzeichen, Zahlen entfernt
- Finale Vokabular-GrÃ¶ÃŸe: **202 einzigartige WÃ¶rter**
- Top-WÃ¶rter: with (9777Ã—), and (6175Ã—), wood (2783Ã—), interior (2371Ã—)

**Spezial-Tokens:**
- `<PAD>` (ID: 0) - AuffÃ¼llen auf feste LÃ¤nge
- `<UNK>` (ID: 1) - Unbekannte WÃ¶rter

**Max. LÃ¤nge:** 128 Tokens (genug fÃ¼r komplexe Beschreibungen)

### 1.2 Text Encoder

**Architektur:**
- 4 Transformer-Schichten
- 256-dimensionale Embeddings
- 8 Attention-Heads
- Positionelle Encodings

**Funktion:**
Der Text-Encoder versteht die **semantische Bedeutung** der WÃ¶rter im Kontext:

```
"house with oak" 
    â†“
[Embedding-Vektor der versteht: 
 - "house" ist das Haupt-Objekt
 - "oak" ist ein Material
 - "with" beschreibt eine Eigenschaft]
```

**Output:** 
- Text-Embeddings: (batch_size, 128, 256)
- Text-Mask: Markiert welche Tokens echt sind (nicht Padding)

---

## ğŸ§± 2. Block-Verarbeitung

### 2.1 Block Vocabulary

**121 verschiedene Minecraft-BlÃ¶cke + 5 Spezial-Tokens:**

**Kategorien:**
- **Natural (34)**: stone, dirt, grass_block, oak_log, spruce_log, sand, ...
- **Building (21)**: stone_bricks, oak_planks, glass, cobblestone, ...
- **Wool/Concrete (32)**: white_wool, red_concrete, blue_terracotta, ...
- **Functional (29)**: oak_door, torch, chest, crafting_table, ...
- **Special (5)**: `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`, `air`

**Warum diese BlÃ¶cke?**
Extrahiert aus den 2415 Trainings-Strukturen - das sind die am hÃ¤ufigsten verwendeten BlÃ¶cke.

### 2.2 Struktur-Format

**16Ã—16Ã—16 = 4096 BlÃ¶cke**

**Speicherformat (Y-Z-X Order):**
```python
# Y = HÃ¶he (0-15, unten nach oben)
# Z = Tiefe (0-15, vorne nach hinten)  
# X = Breite (0-15, links nach rechts)

position = y * 16 * 16 + z * 16 + x
```

**Warum Y-Z-X?**
- Preserviert vertikale ZusammenhÃ¤nge (Schichten)
- Besser fÃ¼r Transformer (nahe BlÃ¶cke = nahe Positionen)
- NatÃ¼rliche Bau-Reihenfolge (von unten nach oben)

---

## ğŸ¤– 3. Das Conditional GPT Modell

### 3.1 Model-Architektur

**GrÃ¶ÃŸe:**
- **Parameter:** ~50-60 Millionen
- **Schichten:** 12 Transformer-Blocks
- **Dimensionen:** 512 (d_model)
- **Attention-Heads:** 8
- **Feed-Forward:** 2048
- **Dropout:** 0.1

**Zwei Hauptkomponenten:**

#### A) Self-Attention (Was wurde schon gebaut?)
```
Jeder neue Block schaut auf alle bisherigen BlÃ¶cke:
"Aha, die letzten 50 BlÃ¶cke waren oak_planks...
 ich sollte das Muster fortsetzen"
```

#### B) Cross-Attention (Was sagt der Text-Prompt?)
```
Der Block schaut auf den Text-Prompt:
"Der Prompt sagt 'medieval house with oak wood'...
 ich sollte oak-bezogene BlÃ¶cke verwenden"
```

### 3.2 Conditional Transformer Block

**Jeder der 12 BlÃ¶cke macht:**

```
1. SELF-ATTENTION
   Input: Bisherige BlÃ¶cke
   Output: "Kontext der Struktur-Geschichte"
   
2. CROSS-ATTENTION  
   Input: Struktur-Kontext + Text-Embeddings
   Output: "Struktur-Kontext konditioniert auf Text"
   
3. FEED-FORWARD
   Input: Konditionierter Kontext
   Output: Finale Block-Representation
```

**In Formeln:**
```python
# Self-Attention: Verstehe die Struktur bisher
x = x + SelfAttention(x)
x = LayerNorm(x)

# Cross-Attention: Kombiniere mit Text-Information
x = x + CrossAttention(query=x, key=text, value=text)
x = LayerNorm(x)

# Feed-Forward: Finales Processing
x = x + FeedForward(x)
x = LayerNorm(x)
```

### 3.3 Autoregressive Generierung

**Block-fÃ¼r-Block Generierung:**

```
Schritt 1: <BOS> â†’ "stone" (Basis-Block)
Schritt 2: <BOS> stone â†’ "stone" (weiter Basis)
Schritt 3: <BOS> stone stone â†’ "cobblestone"
...
Schritt 4096: <BOS> ... â†’ "air" â†’ <EOS>
```

**Warum autogressiv?**
- Jeder Block beeinflusst die nÃ¤chsten
- NatÃ¼rliche Struktur-Entwicklung
- Konsistente Muster

**Sampling-Strategien:**

1. **Greedy (nicht verwendet):**
   ```python
   next_block = argmax(probabilities)
   # â†’ Immer gleiche Struktur
   ```

2. **Temperature Sampling:**
   ```python
   logits = logits / temperature
   # temperature=1.0: Original-Verteilung
   # temperature=0.7: Konservativer (weniger Chaos)
   # temperature=1.3: Kreativer (mehr Variation)
   ```

3. **Top-K Sampling:**
   ```python
   # Nur die top-K wahrscheinlichsten BlÃ¶cke behalten
   top_k = 50  # WÃ¤hle aus den 50 besten
   ```

4. **Top-P (Nucleus) Sampling:**
   ```python
   # WÃ¤hle BlÃ¶cke bis kumulative Prob. >= p
   top_p = 0.95  # Top 95% der Wahrscheinlichkeit
   ```

---

## ğŸ“Š 4. Training-Prozess

### 4.1 Dataset

**Training-Daten:**
- **2415 .schem Dateien** mit Beschreibungen im Dateinamen
- **Split:** 80% Train (1932) / 10% Val (242) / 10% Test (241)
- **Augmentierung:** Rotationen (90Â°, 180Â°, 270Â°), Flips

**Data Loading:**
```python
for each .schem file:
    1. Parse filename â†’ text description
    2. Load .schem â†’ 16x16x16 blocks
    3. Tokenize text â†’ text_ids
    4. Flatten blocks â†’ block_sequence
    5. Create training pair:
       Input:  (text_ids, <BOS> + blocks[:-1])
       Target: (blocks + <EOS>)
```

### 4.2 Training Loop

**Jede Epoche:**
```
FÃ¼r jeden Batch (16 Strukturen):
    1. Encode Text-Prompts
    2. Forward-Pass durch Modell
    3. Berechne Loss (Cross-Entropy)
    4. Backward-Pass (Gradienten)
    5. Update Weights (Adam Optimizer)
```

**Loss-Funktion:**
```python
# Cross-Entropy Loss
loss = CrossEntropyLoss(
    predictions,  # (batch, 4096, 121)
    targets,      # (batch, 4096)
    ignore_index=PAD_TOKEN
)
```

**Optimizer:**
- **AdamW** (Adam mit Weight Decay)
- Learning Rate: 0.0001
- Weight Decay: 0.01
- Gradient Clipping: 1.0 (verhindert Exploding Gradients)

**Learning Rate Schedule:**
- **Cosine Annealing**: LR sinkt sanft Ã¼ber Zeit
- Start: 0.0001 â†’ Ende: ~0.00001

### 4.3 Validation

**Jede Epoche nach Training:**
```python
validation_loss = evaluate(model, val_dataset)

if validation_loss < best_loss:
    save_checkpoint("conditional_model_best.pt")
    best_loss = validation_loss
```

**Warum Validation?**
- Verhindert Overfitting
- FrÃ¼hes Stoppen mÃ¶glich
- Best Model â‰  Final Model

---

## ğŸ’¾ 5. Checkpoint-System

### 5.1 Was wird gespeichert?

```python
checkpoint = {
    'epoch': 42,
    'model_state_dict': {...},  # Alle Weights
    'optimizer_state_dict': {...},  # Optimizer-State
    'loss': 2.34,
}
```

### 5.2 Checkpoint-Typen

1. **conditional_model_best.pt**
   - Niedrigster Validierungs-Loss
   - Meist die beste Wahl fÃ¼r Generation

2. **conditional_model_final.pt**
   - Nach allen Epochen
   - Manchmal overfitted

3. **conditional_model_epoch_N.pt**
   - Alle 10 Epochen
   - Zum Experimentieren / Recovery

---

## ğŸ¯ 6. Generation-Prozess

### 6.1 Von Prompt zu Struktur

```
1. USER INPUT
   "a big medieval house with oak wood"
   
2. TEXT TOKENIZATION
   â†’ [5, 42, 78, 34, 89, 15, 91, 0, 0, ...]  (128 tokens)
   
3. TEXT ENCODING
   â†’ Text-Embeddings (128, 256)
   
4. AUTOREGRESSIVE GENERATION
   Start: [<BOS>]
   Loop 4096 mal:
       a) Predict nÃ¤chsten Block
       b) Sample von Probability-Distribution
       c) Append zu Sequence
   End: [<BOS>, stone, stone, oak_planks, ..., air, <EOS>]
   
5. RESHAPE & SAVE
   â†’ 16Ã—16Ã—16 Array
   â†’ .schem Datei
```

### 6.2 Generation-Parameter

**Temperature (0.5 - 2.0):**
```
Low (0.7):  Konservativ, nah am Training
            â”œâ”€ stone (70%)
            â”œâ”€ cobblestone (20%)
            â””â”€ oak_planks (10%)

Medium (1.0): Ausgewogen
              â”œâ”€ stone (40%)
              â”œâ”€ cobblestone (30%)
              â””â”€ oak_planks (30%)

High (1.3):  Kreativ, experimentell
             â”œâ”€ stone (25%)
             â”œâ”€ cobblestone (25%)
             â”œâ”€ oak_planks (25%)
             â””â”€ spruce_planks (25%)
```

**Top-K (0 = disabled, 20-100):**
```
top_k = 50
â†’ WÃ¤hle nur aus den 50 wahrscheinlichsten BlÃ¶cken
â†’ Verhindert total unrealistische BlÃ¶cke
```

**Top-P (0 = disabled, 0.9-0.99):**
```
top_p = 0.95
â†’ WÃ¤hle BlÃ¶cke bis 95% kumulative Wahrscheinlichkeit
â†’ Dynamische Auswahl (mehr bei unsicheren Stellen)
```

---

## ğŸ“ˆ 7. Metriken & Bewertung

### 7.1 Training-Metriken

**Loss (Cross-Entropy):**
- Misst wie gut das Modell den nÃ¤chsten Block vorhersagt
- Niedriger = besser
- Typisch: Start ~4.5 â†’ Ende ~2.0-2.5

**Perplexity:**
```python
perplexity = exp(loss)
```
- Interpretierbar als "Verwirrung" des Modells
- Niedriger = besser

### 7.2 QualitÃ¤ts-Bewertung

**Schwierig automatisch zu bewerten!** Aber:

1. **Block-Diversity:**
   ```python
   unique_blocks / total_blocks
   # Zu niedrig: Langweilig (nur stone)
   # Zu hoch: Chaotisch (random BlÃ¶cke)
   ```

2. **Struktur-KohÃ¤renz:**
   - Verbundene WÃ¤nde?
   - Dach Ã¼ber RÃ¤umen?
   - Sinnvolle Material-Kombinationen?

3. **Prompt-Adherence:**
   - "with oak wood" â†’ Viel oak_planks?
   - "stone base" â†’ Untere Schichten = stone?

4. **Menschliche Bewertung:**
   - Sieht es aus wie ein GebÃ¤ude?
   - Ist es dem Prompt Ã¤hnlich?
   - WÃ¼rdest du es in Minecraft bauen?

---

## ğŸ”¬ 8. Technische Details

### 8.1 Attention-Mechanismus

**Self-Attention (Vereinfacht):**
```python
# FÃ¼r jeden Block:
for i in range(current_position):
    # Wie wichtig ist Block i fÃ¼r die Vorhersage?
    attention_weight = softmax(
        query[current] @ key[i]
    )
    
    # Gewichtete Summe aller vorherigen BlÃ¶cke
    context += attention_weight * value[i]
```

**Cross-Attention:**
```python
# FÃ¼r jeden Block:
for text_token in text_embeddings:
    # Wie relevant ist dieses Text-Token?
    attention_weight = softmax(
        query[current_block] @ key[text_token]
    )
    
    # Integriere Text-Information
    context += attention_weight * value[text_token]
```

### 8.2 Positional Encoding

**Warum?**
Transformer hat keine inhÃ¤rente Positions-Information.

**Wie?**
```python
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

- pos = Position im Sequence
- i = Dimensions-Index
- Einzigartig fÃ¼r jede Position
- ErmÃ¶glicht Interpolation fÃ¼r ungesehene LÃ¤ngen

### 8.3 Layer Normalization

**Stabilisiert Training:**
```python
normalized = (x - mean(x)) / sqrt(var(x) + epsilon)
output = gamma * normalized + beta
```

- Verhindert Exploding/Vanishing Activations
- ErmÃ¶glicht tiefere Netzwerke
- Schnelleres Konvergieren

### 8.4 Dropout

**Regularisierung:**
```python
# Training: ZufÃ¤llig 10% der Neuronen deaktivieren
if training:
    x = x * (random() > 0.1)
```

- Verhindert Overfitting
- Zwingt Netzwerk zu robusteren Features
- Nur wÃ¤hrend Training, nicht bei Generation

---

## âš¡ 9. Performance-Optimierungen

### 9.1 Batch Processing

**Parallel verarbeiten:**
```
Batch-Size 16:
â†’ 16 Strukturen gleichzeitig
â†’ GPU-Auslastung â†‘
â†’ Training schneller
```

**Trade-off:**
- GrÃ¶ÃŸer: Schneller, aber mehr Memory
- Kleiner: Langsamer, aber weniger Memory

### 9.2 Mixed Precision Training

**FP16 statt FP32:**
```python
# Halbiert Memory-Verwendung
# ~2x schneller Training
# Minimal Loss in Genauigkeit
```

(Aktuell nicht implementiert, aber mÃ¶glich)

### 9.3 Gradient Accumulation

**FÃ¼r groÃŸe Batch-Sizes:**
```python
# Simuliert Batch-Size 64 mit nur 16:
for i in range(4):
    loss = forward(batch[i])
    loss.backward()  # Akkumuliert Gradienten
    
optimizer.step()  # Update nach 4 Batches
```

### 9.4 Data Loading

**Parallel & Pinned Memory:**
```python
DataLoader(
    num_workers=4,      # 4 Threads laden Daten
    pin_memory=True,    # Schnellerer GPU-Transfer
    prefetch_factor=2   # LÃ¤dt im Voraus
)
```

---

## ğŸ§ª 10. Limitierungen & Future Work

### 10.1 Aktuelle Limitierungen

1. **Fixe GrÃ¶ÃŸe:** Nur 16Ã—16Ã—16
   - GrÃ¶ÃŸere Strukturen nicht mÃ¶glich
   - Kleine Details schwierig

2. **Limitiertes Vokabular:** 202 Text-WÃ¶rter
   - Neue Konzepte nicht verstanden
   - Beschreibungen mÃ¼ssen Ã¤hnlich zu Training sein

3. **Keine Garantien:**
   - Kann "unmÃ¶gliche" Strukturen erzeugen
   - Schwebende BlÃ¶cke mÃ¶glich
   - Physik nicht berÃ¼cksichtigt

4. **Lange Generierung:** ~30 Sekunden pro Struktur
   - Autoregressive = langsam
   - 4096 sequential steps

5. **Training-Daten Bias:**
   - Nur mittelalterliche/Fantasy-Stile
   - Keine modernen GebÃ¤ude
   - Material-Kombinationen aus Training

### 10.2 MÃ¶gliche Verbesserungen

**GrÃ¶ÃŸere Strukturen:**
```python
# 32Ã—32Ã—32 oder 64Ã—64Ã—64
# BenÃ¶tigt: Mehr Memory, lÃ¤ngeres Training
```

**Hierarchical Generation:**
```python
# 1. Grobe Struktur (8Ã—8Ã—8)
# 2. Verfeinern zu (16Ã—16Ã—16)
# 3. Details hinzufÃ¼gen (32Ã—32Ã—32)
```

**Conditional VAE:**
```python
# Latent-Space Manipulation
# Interpolation zwischen Strukturen
# Stil-Transfer
```

**Multi-Scale Attention:**
```python
# Lange Distanz-Dependencies
# Globale Struktur-KohÃ¤renz
# Schnellere Generierung
```

**Reinforcement Learning:**
```python
# Reward fÃ¼r:
# - Physikalisch stabil
# - Ã„sthetisch ansprechend
# - Prompt-Adherence
```

---

## ğŸ“š 11. Mathematische Formeln

### 11.1 Transformer Attention

**Scaled Dot-Product Attention:**
```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V

Q = Query Matrix (Was suche ich?)
K = Key Matrix (Was habe ich?)
V = Value Matrix (Was ist der Wert?)
d_k = Dimensions-GrÃ¶ÃŸe
```

**Multi-Head Attention:**
```
MultiHead(Q,K,V) = Concat(head_1,...,head_h)Â·W^O

head_i = Attention(QÂ·W_i^Q, KÂ·W_i^K, VÂ·W_i^V)

h = Anzahl Heads (8)
```

### 11.2 Loss-Funktion

**Cross-Entropy Loss:**
```
L = -âˆ‘_{t=1}^{4096} log P(b_t | b_{<t}, text)

b_t = Ground-Truth Block an Position t
P() = Model-Wahrscheinlichkeit
b_{<t} = Alle vorherigen BlÃ¶cke
```

### 11.3 Sampling

**Temperature Scaling:**
```
P_i = exp(logit_i / T) / âˆ‘_j exp(logit_j / T)

T = Temperature
Tâ†’0: Deterministisch (argmax)
T=1: Original-Distribution
Tâ†’âˆ: Uniform random
```

---

## ğŸ¯ Zusammenfassung

**Kernidee:**
Die KI kombiniert **Natural Language Understanding** (Text-Encoder) mit **Autoregressive Generation** (GPT) und **Cross-Attention**, um Minecraft-Strukturen zu erzeugen, die zu Text-Beschreibungen passen.

**Warum funktioniert es?**
1. **GroÃŸe Trainings-Daten:** 2415 Strukturen mit Labels
2. **Starke Architektur:** Transformer mit 50M+ Parametern
3. **Conditioning:** Cross-Attention verbindet Text â†” Struktur
4. **Autoregressive:** Jeder Block baut auf vorherigen auf

**Key Innovation:**
Die Kombination von Text-Conditioning mit strukturierter 3D-Generierung. Nicht einfach "Text â†’ Bild", sondern "Text â†’ 3D-Voxel-Struktur mit Spatial Coherence".

---

**FÃ¼r weitere Fragen zur Nutzung:** Siehe `TUTORIAL.md`
